{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://web-scraping.dev/', '#', '/docs', '/api/graphql', 'https://web-scraping.dev/products', 'https://web-scraping.dev/reviews', 'https://web-scraping.dev/testimonials', 'https://web-scraping.dev/file-download', 'https://web-scraping.dev/login', '/cart', 'https://web-scraping.dev/products?category=apparel', 'https://web-scraping.dev/products?category=consumables', 'https://web-scraping.dev/products?category=household', 'https://web-scraping.dev/product/1', 'https://web-scraping.dev/product/2', 'https://web-scraping.dev/product/3', 'https://web-scraping.dev/product/4', 'https://web-scraping.dev/product/5', 'https://web-scraping.dev/products?page=1', 'https://web-scraping.dev/products?page=2', 'https://web-scraping.dev/products?page=3', 'https://web-scraping.dev/products?page=4', 'https://web-scraping.dev/products?page=5', 'https://web-scraping.dev/products?page=2', 'https://scrapfly.io/academy', '/docs', '/sitemap.xml', 'https://scrapfly.io/blog', 'https://github.com/scrapfly', 'https://web-scraping.dev/']\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.robotparser\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Développer une fonction qui s’assure que le crawler a le droit de parser une page\n",
    "\n",
    "def check_politess(url):\n",
    "    url_robots = url + fr\"robots.txt\"\n",
    "    # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "    response = requests.get(url_robots) # ,headers\n",
    "    liste = response.text.split('\\n\\n')\n",
    "    nos_permissions = [ el for el in liste if el.startswith('User-Agent: *')]\n",
    "    #print(nos_permissions[0].split('\\n'))\n",
    "    #print(response.text)\n",
    "    return True # pour l'instant\n",
    "\n",
    "\n",
    "def get_link(soup_detail):\n",
    "    body = soup_detail.find('body')\n",
    "    balises_avec_link_interessant = body.find_all('a',href=True)\n",
    "    links=[]\n",
    "    for content in balises_avec_link_interessant:\n",
    "        links.append(content['href'])\n",
    "    return links\n",
    "\n",
    "def html_parser(url):\n",
    "    if check_politess(url):\n",
    "        response = requests.get(url+rf'/products')\n",
    "        soup = BeautifulSoup( response.text , 'html.parser')\n",
    "        links = get_link(soup)\n",
    "        print(links)\n",
    "\n",
    "        #print(soup.prettify())\n",
    "\n",
    "\n",
    "\"\"\" dans les listes de sliens , il faut supprimer ceux avce category , car on checrje dans all donc on fait paa deux fois le meme travail\"\"\"\n",
    "\n",
    "url='https://web-scraping.dev/'\n",
    "check_politess('https://web-scraping.dev/')\n",
    "html_parser(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0face98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indulge your sweet tooth with our Box of Chocolate Candy. Each box contains an assortment of rich, flavorful chocolates with a smooth, creamy filling. Choose from a variety of flavors including zesty orange and sweet cherry. Whether you're looking for the perfect gift or just want to treat yourself, our Box of Chocolate Candy is sure to satisfy.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.robotparser\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Développer une fonction qui s’assure que le crawler a le droit de parser une page\n",
    "\n",
    "def check_politess(url):\n",
    "    url_robots = url + fr\"robots.txt\"\n",
    "    # headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "    response = requests.get(url_robots) # ,headers\n",
    "    liste = response.text.split('\\n\\n')\n",
    "    nos_permissions = [ el for el in liste if el.startswith('User-Agent: *')]\n",
    "    #print(nos_permissions[0].split('\\n'))\n",
    "    #print(response.text)\n",
    "    return True # pour l'instant\n",
    "\n",
    "def get_title(soup):\n",
    "    title = soup.find('meta',property=\"og:title\")\n",
    "    return title['content']\n",
    "\n",
    "def get_description(soup):\n",
    "    title = soup.find('p',class_=\"product-description\")\n",
    "    print(title.text)\n",
    "\n",
    "def html_parser(url):\n",
    "    if check_politess(url):\n",
    "        response = requests.get(url+rf'/product/1')\n",
    "        soup = BeautifulSoup( response.text , 'html.parser')\n",
    "\n",
    "        #print(soup.prettify())\n",
    "\n",
    "\n",
    "\"\"\" dans les listes de sliens , il faut supprimer ceux avce category , car on checrje dans all donc on fait paa deux fois le meme travail\"\"\"\n",
    "\n",
    "# Implémenter une file d'attente des URLs à visiter\n",
    "def liens_a_vister():\n",
    "    pass\n",
    "\n",
    "url='https://web-scraping.dev/'\n",
    "html_parser(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ee118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_parser(url):\n",
    "    if check_politess(url):\n",
    "        response = requests.get(url+rf'/product/1')\n",
    "        soup = BeautifulSoup( response.text , 'html.parser')\n",
    "\n",
    "        #print(soup.prettify())\n",
    "\n",
    "\n",
    "\"\"\" dans les listes de sliens , il faut supprimer ceux avce category , car on checrje dans all donc on fait paa deux fois le meme travail\"\"\"\n",
    "\n",
    "# Implémenter une file d'attente des URLs à visiter\n",
    "\n",
    "set_lien_deja_visite={}\n",
    "def liens_a_vister(liste_link):\n",
    "    pass\n",
    "\n",
    "url='https://web-scraping.dev/'\n",
    "html_parser(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
